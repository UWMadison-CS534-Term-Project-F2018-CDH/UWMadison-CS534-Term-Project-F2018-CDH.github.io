<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>CNN Sprite Generator</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Generating new 2D game assests using DC-GAN.">
    <link rel="canonical" href="http://localhost:4000/">

    <!-- Custom CSS & Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link rel="stylesheet" href="/style.css">

    <!-- Custom Fonts -->
    <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>
    <body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">
     <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <i class="fa fa-play-circle"></i>  <span class="light">CS 534 Term Project</span> CNN Sprite Generator
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#summary">Summary</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#download">Download</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#project">Project</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

        <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h1 class="brand-heading">CNN Sprite Generator</h1>
                        <p class="intro-text">Generating new 2D game assests using DC-GAN.<br>
                        <a href="#summary" class="btn btn-circle page-scroll">
                            <i class="fa fa-angle-double-down animated"></i>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

        <!-- Summary Section -->
    <section id="summary" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Summary</h2>
                <p class=MsoNormal><span lang=EN>A problem in video game development is the
                amount of novel content that needs to be generated. Inspired by recent work
                from Horslev &amp; Perez-Liebana, we developed a deep convolutional generative
                adversarial network (DCGAN) to generate 2D video game sprites. Our model was
                augmented with a convolutional autoencoder that was pre-trained to render
                sprites from a feature stack. Our GAN would then learn the feature stack to
                produce novel sprites. While we observed a decrease in training time to initial
                reasonable results the approach traded in stability for that speed. Overall our
                results show promise at creating novel sprites but needs further work to solve
                the stability issue.</span></p>
        </div>
    </section>

        <!-- Download Section -->
    <section id="download" class="content-section text-center">
        <div class="download-section">
            <div class="container">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Download</h2>
                    <p>Source code from GitHub</p>
                    <a href="https://github.com/UWMadison-CS534-Term-Project-F2018-CDH" target="_blank" class="btn btn-default btn-lg">Visit GitHub</a>

                    <p><br>Course related documents provided</p>
                    <p>
                        <a href="/assets/files/proposal.pdf" class="btn btn-default btn-lg" download>Proposal</a>
                        <a href="/assets/files/mid-term-report.pdf" class="btn btn-default btn-lg" download>Mid-term Report</a>
                        <a href="/assets/files/presentation.pdf" class="btn btn-default btn-lg" download>Final Presentation</a>
                    </p>

                    <p><br>Datasets</p>
                    <p>
                        <a href="/assets/datasets/all.zip" class="btn btn-default btn-lg" download>All Sprites</a>
                        <a href="/assets/datasets/effects.zip" class="btn btn-default btn-lg" download>Effect Sprites</a>
                        <a href="/assets/datasets/entities.zip" class="btn btn-default btn-lg" download>Entity Sprites</a>
                        <a href="/assets/datasets/environment.zip" class="btn btn-default btn-lg" download>Environment Sprites</a>
                        <a href="/assets/datasets/gui.zip" class="btn btn-default btn-lg" download>GUI Sprites</a>
                        <a href="/assets/datasets/items.zip" class="btn btn-default btn-lg" download>Item Sprites</a>
                        <a href="/assets/datasets/skills.zip" class="btn btn-default btn-lg" download>Skill Sprites</a>
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Project Section -->
<section id="project" class="container content-section text-center">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
          <h2>Project</h2>
        </div>
        <div class="col-lg-8 col-lg-offset-0" style="display: inline-block; text-align: left;">
          <div class=WordSection1>

            <h3><a name="_cnvzf0rtdyob"></a><span lang=EN>Motivation</span></h3>

            <p class=MsoNormal><span lang=EN>Developing visual content for games, even
            simplified 2D games, proves to be a hurdle for indie studios and solo game
            developers who have neither the time nor capital to develop captivating
            environments, engaging character designs, and remarkable particle effects. So,
            these developers turn to procedural algorithms to assist them in this endeavor.
            Current, general-use algorithms either iterate over the equivalent of building
            blocks to construct sprites that lack the detail of a hand-crafted image or
            explore a crafted algorithm using a pseudo-random seed [1]. Alternatively, a
            developer can develop a deep neural network, such as a generative-adversarial
            network, to produce novel sprites for their game [2]. However, a neural network
            is not a panacea; it requires a large dataset of images. Additionally, the
            developer must rely on the network converging to a optimal weight space while
            simultaneously not overfitting.</span></p>

            <h3><a name="_ekyaqsbfigp3"></a><span lang=EN>Approach</span></h3>

            <p class=MsoNormal><span lang=EN>Our solution takes significant inspiration
            from Lewis Horsley's and Diego Perez-Liebana's 2017 paper titled <i>Building an
            Automatic Sprite Generator with Deep Convolutional Generative Adversarial
            Networks </i>[2]. The rational to use a GAN over hand-crafting would be to
            generate sprites without much human intervention after model training. Note
            further refinement and animation would still need to be done to tailor these
            sprites for the specific game. As a future step the sprite pipeline may include
            animation generation [5]. Our motivation partially drawn from personal experience
            where the problem with making games comes down to doing the art and partially
            inspired from literature [1]. </span></p>

            <p class=MsoNormal><span lang=EN>One valid critique of the GAN method is the
            sprite's art style. That is the sprite will be a combination of features from
            the dataset the network was trained on and thus may not have consistent
            artistic qualities. Our solution will handle this in two parts. First, the
            dataset will be curated with a rough theme in mind, something analogous to a
            game studio's genre. Second, themes will be taught into an autoencoder as it
            can be thought of as a pre-trained style domain that learned the visual
            metaphor for sprite generation in its decoder. </span></p>

            <h3><a name="_ncjdnnop30v8"></a><span lang=EN>Implementation</span></h3>

            <p class=MsoNormal><span lang=EN>Our solution will first train an autoencoder,
            then flip the encoder, decoder using the hidden central feature structure as
            both input and output, see Figure 1. After this we will hold the autoencoder's
            weights constant while training a generator and discriminator. Meaning the GAN
            only needs to learn what features to select and discriminate on instead of
            having to learn the entire space. Note a possible limitation of our solution is
            a potential for loss in novelty with the worst-case being mode collapse [6]. </span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=305 id=image5.png src="/assets/images/image001.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            1: High-level network design with autoencoder.</span></i></p>

            <p class=MsoNormal><span lang=EN>Drawing on Lewis Horsley's and Diego
            Perez-Liebana's paper [2], we will use a similar DCGAN [4] model including
            their use of strided convolution in lieu of maxpooling and fractional strided
            convolution instead of upsampling and we will implement batch normalization.
            Additionally, we are making the following changes,</span></p>

            <div style="padding-left: .5in">
            <p class=MsoListParagraphCxSpFirst style='text-indent:-.25in'><span lang=EN>1.<span>&nbsp;&nbsp; </span></span><span
            lang=EN>Our network will generate images of 32x32x4 pixel images.</span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>2.<span>&nbsp; </span></span><span
            lang=EN>We will adjust the hyperparameters such as
               learning rate and network structure to get best possible results from our
               dataset.</span></p>

            <p class=MsoListParagraphCxSpLast style='text-indent:-.25in'><span lang=EN>2.<span>&nbsp; </span></span><span
            lang=EN>We will perform pretraining of the generator
              model through autoencoders. Our network will use successive decoders from
              result back trained through the normal unsupervised process. This should
              populate the weight with a better chance of generalizing from input. A
              potential drawback will be lack of novelty in output images if the
              autoencoder overfits during training.</span></p>
            </div>

            <p class=MsoNormal><span lang=EN>The following images detail the the model
            architecture as sub-networks that correspond to the high-level image. The
            generator takes an input vector with 100 dimensions into a fully connected
            layer that gets reshaped into feature stack shown. The next two layers are
            transpose convolutional layers that learn to upsample the features to the form
            that the decoder takes. Activation functions are ReLU.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN
            style='font-size:18.0pt;line-height:115%'><img width=624 height=161
            id=image15.png src="/assets/images/image002.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            2: Generator sub-network.</span></i></p>


            <p class=MsoNormal><span lang=EN>The decoder performs transpose convolution to
            move from feature space to the final image. This sub-network has its weights
            frozen while training the GAN as it is pre-trained in the autoencoder step.
            Activation functions are ReLU.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=276 id=image4.png src="/assets/images/image003.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            3: Decoder sub-network.</span></i></p>


            <p class=MsoNormal><span lang=EN>Likewise, the encoder has its weights froze
            while training the GAN as it too comes from the autoencoder step. The goal of
            this network is to learn a feature stack from an image using convolutional
            layers. Activation functions are ReLU except the last layer which has a sigmoid
            activation function.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=229 id=image12.png src="/assets/images/image004.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            4: Encoder sub-network.</span></i></p>


            <p class=MsoNormal><span lang=EN>Finally, the discriminator sub-network takes
            the feature vector generated from the encoder and through convolutional layers
            determines whether the image is real or fake. The final output is a dense
            layer, though it is only a single element. Activation functions are leaky ReLU
            with 0.2 leakage except the last layer which has a sigmoid activation function.</span></p>


            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=123 id=image10.png src="/assets/images/image005.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            5: Discriminator sub-network.</span></i></p>

            <h3><a name="_xjlqujpekft0"></a><span lang=EN>Comparison Architecture</span></h3>

            <p class=MsoNormal><span lang=EN>For comparison we will develop a standard
            DCGAN that closely follow Horsley's and Perez-Liebana's architecture. without
            the autoencoder and labeling. This network will still have hyperparameters
            adjusted to produce best possible output given the limitation on time.</span></p>

            <h3><a name="_yfrk64blmab2"></a><span lang=EN>Datasets</span></h3>

            <p class=MsoNormal><span lang=EN>Regarding the dataset, we will be collecting
            sprites from open source resources, specifically OpenGameArt.org. This should
            provide enough data for an initial system. If necessary, we can perform simple
            manipulations such as changing the color palette and single-pixel
            manipulations. Finally, if we need more data than what can be collected online
            and with time permitting, we will employ neural style transfer on our current
            sprite. Our team recognizes the largest risk to the success of the project is
            collecting enough quality data for our network to generalize. Our models will
            be trained on constrained topics as subsets of items, entities, and
            environment. While this will reduce our training set it will also reduce the
            variance thereby reducing cases to learn.</span></p>



            <p class=MsoNormal><span lang=EN>All sprite data will be filtered to only
            include sprites of 32x32 pixels, some sprites may be cropped or padded (note
            sprites that when manipulated, lose the original meaning such as terrain, will
            be discarded). Sprites themselves may either originate from a sprite-sheet
            where they are extracted or may already be provided individually. The sprite is
            then normalized to a four-channel image (RGBA) before being manually classified
            in a hierarchy. In the future a classification neural network could be
            developed for sorting but that is outside the scope of our project. Finally, we
            package the sorted image data as serialized Numpy arrays along with labels
            generated from the hierarchical structure due to sorting.</span></p>

            <h4><a name="_fcple86q6udd"></a><span lang=EN>Development and Training</span></h4>

            <p class=MsoNormal><span lang=EN>We are using Keras to develop both our
            autoencoder architecture and the comparison architecture. Keras was chosen due
            to the ease of use in leveraging Tensorflow and that it is a Python library.
            For each subset of images, the class of interest, we will train a new variant
            of the network. Regarding training we chose Google Colab as it provides free
            GPU access for a Jupyter notebook.</span></p>

            <h3><a name="_aey3jvgiru5g"></a><span lang=EN>Results</span></h3>

            <p class=MsoNormal><span lang=EN>In the following sections we will cover our
            development process starting with the GAN created to generate handwritten
            digits, the comparison GAN, and autoencoder GAN. Each section will detail the
            design decisions and inspiration behind it along with several result images.</span></p>

            <h4><a name="_96l24uf8g4in"></a><span lang=EN>GAN - MNIST Handwritten Digits</span></h4>

            <p class=MsoNormal><span lang=EN>As a proof of concept while collecting our
            sprite dataset we developed a GAN to generate handwritten digits. This allowed
            us to get familiar with Keras and develop the initial GAN architecture.' Figure
            6 displays several digits generated from the trained network. These results
            were generated after training the network for 20 epochs using the full MNIST
            dataset.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=612 height=618 id=image7.png src="/assets/images/image006.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            6: MNIST handwritten digits generated by GAN.</span></i></p>

            <h4><a name="_wuqh5r3gnh95"></a><span lang=EN>Comparison GAN</span></h4>

            <p class=MsoNormal><span lang=EN>Using an exploratory GAN based off the model
            architecture developed for the MNIST problem, we trained a network that
            generated terrain tiles, Figure 7. Our network was trained for approximately
            2000 epochs to achieve these results. Most interesting sprites are the various
            block patterns that emerge as a feature. Note that some of the artifacts are
            exaggerated due to the sprites being scaled, however there is still much work
            to be done in generating valid sprites.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=624 id=image21.png src="/assets/images/image007.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            7: Environment sprites generated with the network created. Note that some of
            the blurring artifacts are generated due to scaling the 32x32 pixels.</span></i></p>

            <p class=MsoNormal><span lang=EN>Using a similar network structure to the model
            that generated environmental sprites, we trained a GAN that could generate
            items, specifically our largest subset was various weapons like swords, staffs,
            maces, etc. Figure 8 shows what our network learned at approximately 500
            epochs. A thousand epochs later, the results are shown in Figure 9. In both
            cases the network has difficulty learning the relevant details but has a
            general understanding of shape and shadow.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=629 id=image16.png src="/assets/images/image008.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            8: Weapon sprites generated with GAN. Note results are poor but understandable.</span></i></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=629 id=image11.png src="/assets/images/image009.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            9: Results generated after further training of model. Improvement shown but
            quality in terms of color, but it is still low.</span></i></p>


            <p class=MsoNormal><span lang=EN>Further modification to the DCGAN along with
            training the network for 3000 epochs resulted in a model that seemed to produce
            some valid results. Figure 10, shows a sample of these generated images, note
            that the network did not generalize well, instead learning only the exemplars
            in the dataset. For example, the sword generated in the center is almost
            identical to one training sprite. We attempt address this issue by increasing
            our dataset and thus variance. </span></p>


            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=629 id=image23.png src="/assets/images/image010.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            10: Potential overfitting of the limited weapons dataset after modifying
            network structure. Note that this set was randomly selected, the duplicates
            appeared by chance.</span></i></p>

            <h5><a name="_jgxgvhseiur9"></a><span lang=EN>Training</span></h5>

            <p class=MsoNormal><span lang=EN>Below are some of the recent training results
            captured for the comparison GAN. In these we were training entities and armor.
            While the color is off after the training, along with partial mode collapse,
            the results to produce new variants of the sprites within the dataset.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN
            style='font-size:18.0pt;line-height:115%'><img width=624 height=624
            id=image25.gif src="/assets/images/image011.gif"></span></i></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            11: Human-like entities being learned.</span></i></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=624 id=image19.gif src="/assets/images/image012.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            12: Humanoid being learned.</span></i></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN><img
            width=624 height=624 id=image20.gif src="/assets/images/image013.gif"></span></i></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            13: Torso armor being learned.</span></i></p>


            <h4><a name="_mjhfgc57seey"></a><span lang=EN>Autoencoder GAN</span></h4>

            <p class=MsoNormal><span lang=EN>We started with one layer of autoencoder which
            produced good results as shown below, Figure 14. However, the network could not
            learn with a second layer of autoencoder with any reasonable accuracy. Thus,
            for our architecture we pretrain one layer of autoencoder before swapping order
            of encoder and decoder.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN
            style='font-size:18.0pt;line-height:115%'><img width=250 height=249
            id=image14.png src="/assets/images/image014.gif"><img width=250
            height=249 id=image8.gif src="/assets/images/image015.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            14: Original vs Autoencoder generated.</span></i></p>

            <p class=MsoNormal><span lang=EN>Using the pre-trained autoencoder, we trained
            our architecture GAN in the same manner as the comparison network. The GAN,
            with the pre-trained autoencoder, suffered from partial mode collapse. The
            generator outputs lacked in diversity due to the generator jumping from one
            single optimal point to another.</span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=629 id=image1.png src="/assets/images/image016.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            15: Mode collapse. The generated sprites exhibit low variance.</span></i></p>

            <p class=MsoNormal style='text-align:justify'><span lang=EN>The generator
            outputs showed little variance in structure, however, the variance in color was
            more pronounced. Comparing the generator and discriminator losses between our
            autoencoder and comparison GANs brought up another issue with our autoencoder
            GAN. </span></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=604 height=339 id=image17.png src="/assets/images/image017.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            16: Loss plot for comparison GAN.</span></i></p>

            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=601 height=339 id=image24.png src="/assets/images/image018.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            17: Loss plot for Autoencoder GAN.</span></i></p>

            <p class=MsoNormal style='text-align:justify'><span lang=EN>The discriminator
            and generator losses, in the comparison GAN, do indicate that both the
            generator and discriminator are able to optimize their loss function without
            one overpowering the other. However, in out autoencoder GAN, the discriminator
            loss remains close to zero throughout training. With the discriminator giving
            values close to 0 or 1, the generator will struggle to read the gradient. One
            theory as to why this is happening is that the encoder knowing the pattern may
            make the determination of a 'real' or 'fake' sprite (by the discriminator)
            easier. </span></p>

            <p class=MsoNormal style='text-align:justify'><span lang=EN>The autoencoder GAN
            loss plot confirmed the network was suffering from mode collapse. The sharps drop
            in generator loss and slight increase is discriminator may signify the
            generator converging to a single optimal point. The discriminator then learns
            to classify these generated images as fake and the generator will collapse to
            the next optimal point.</span></p>

            <p class=MsoNormal style='text-align:justify'><span lang=EN>The simplest
            solution to implement in this case was to increase the batch size while
            training the GAN. This solution worked to some extent, when training on large
            datasets. The resulting outputs had more variance and novelty.</span></p>

            <p class=MsoNormal style='text-align:justify'><span lang=EN>' </span></p>


            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=624 height=629 id=image6.png src="/assets/images/image019.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            18: Autoencoder GAN results.</span></i></p>


            <p class=MsoNormal style='text-align:justify'><span lang=EN>This however did
            not prove to be a great solution as the autoencoder GAN did not perform well
            for limited datasets. Also, even on large datasets, once the GAN experience
            partial collapse the training process had to be restarted. Nevertheless, the
            autoencoder GAN did show promise through the novelty of the outputs and a
            relatively lower training time. </span></p>



            <p class=MsoNormal><span lang=EN>A final attempt to handle the discriminator
            loss approaching zero was to use the comparison GAN's discriminator and our
            model's generator and decoder. This produced the following sprites which was an
            minor improvement over our previous autoencoder models.</span></p>



            <p class=MsoNormal align=center style='text-align:center'><span lang=EN><img
            width=183 height=161 id=image22.png src="/assets/images/image020.gif"><img
            width=180 height=168 id=image3.png src="/assets/images/image021.gif"><img
            width=161 height=151 id=image2.png src="/assets/images/image022.gif"><img
            width=210 height=195 id=image9.png src="/assets/images/image023.gif"><img
            width=170 height=191 id=image18.png src="/assets/images/image024.gif"><img
            width=231 height=191 id=image13.png src="/assets/images/image025.gif"></span></p>

            <p class=MsoNormal align=center style='text-align:center'><i><span lang=EN>Figure
            19: Sprites generated from decoder only with comparison discriminator.</span></i></p>

            <h3><a name="_dv85wmly1rwm"></a><span lang=EN>Discussion</span></h3>

            <p class=MsoNormal style='margin-bottom:16.0pt'><span lang=EN>In order to
            further progress in the topic of sprite generation we implemented a generative
            adversarial network that was augmented with a pre-trained autoencoder. Thus,
            instead of learning to draw images, the GAN was responsible for learning the
            feature space of an autoencoder that was pre-trained to render sprites. This
            resulted in a model that could learn novel pattern with the relatively small
            dataset we collected. Furthermore, we found that using an autoencoder decreased
            time to get initial results but at a major trade-off of stability during training.
            This was observed by the GAN's discriminator approaching zero early in the
            training. Several possible solutions we have found include minibatch
            discrimination, using multiple models to train different for different modes,
            unrolled GANs, and collecting a larger dataset of sprites.</span></p>

            <p class=MsoNormal style='margin-bottom:16.0pt'><span lang=EN>A second
            challenge encountered during development was generating images with correct
            coloring and alpha channel. For example, the autoencoder trained on a small
            dataset can produce the correct shape but would apply a new color palette to
            the result image. Likewise, both the comparison GAN and the autoencoder GAN
            would fail to learn color information while training on occasion. For the
            autoencoder this resulted in sprites with inconsistent color palette and in the
            comparison model the sprites would have color noise. These pixel errors
            jeopardize the visual metaphor that the sprite is supposed to serve if used in
            a game. We believe the first step to addressing this problem is to build a
            larger dataset with control for variance.</span></p>

            <p class=MsoNormal style='margin-bottom:16.0pt'><span lang=EN>The next step of
            the project is to explore alternate implementations, strategies, etc. to
            further test the autoencoder concept. Near the end of the term we started work
            on a conditional GAN with the hope that this handles the variance in the
            dataset. However, we repeatedly had the discriminator loss approach zero early
            in training (without the autoencoder). Thus, we abandoned that aspect of the
            project scope to focus on what was working. It would be a logical step to
            attempt this again. Another route that the autoencoder GAN could go is to
            generate grayscale images with a separate model learning to recolor a sprite.
            This would leverage the strength of the autoencoder as we observed (that is
            learning sprite structure) without the observed weakness. </span></p>

            <p class=MsoNormal style='margin-bottom:16.0pt'><span lang=EN>While the
            setbacks due to stability and color challenge the benefit of the autoencoder,
            they do not necessarily invalidate the approach. We believe the potential
            benefit as a method to reduce learning time and increase novelty is still
            suggested by the results, we were able to generate. Further work is needed to
            triangulate these results. </span></p>

            <h3><a name="_hfsmf67rmmxy"></a><span lang=EN>Acknowledgements</span></h3>

            <h5 class=MsoNormal><span lang=EN>Website Template</span></h5>

            <div style="padding-left: .5in">
            <p class=MsoListParagraphCxSpFirst style='text-indent:-.25in'><span lang=EN>1.<span
            style="font:7.0pt">&nbsp;&nbsp; </span></span><span
            lang=EN>Jeromelachaud. Grayscale-theme. License:
               Apache-2.0. Online: <u><span><a
            href="https://opengameart.org/content/496-pixel-art-icons-for-medievalfantasy-rpg"><span
            >https://github.com/jeromelachaud/grayscale-theme</span></a></span></u></span></p>
            </div>

            <br>

            <h5 class=MsoNormal><span lang=EN>Sprite Dataset Contributions</span></h5>

            <div style="padding-left: .5in">
            <p class=MsoListParagraphCxSpFirst style='text-indent:-.25in'><span lang=EN>1.<span
            >&nbsp;&nbsp; </span></span><span
            lang=EN>Henrique Laxarini (7Soul1). 496 Pixel Art Icons for Medieval/Fantasy
            RPG. License: CC0 - Public Domain. Online: <u><span><a
            href="https://opengameart.org/content/496-pixel-art-icons-for-medievalfantasy-rpg"><span
            >https://opengameart.org/content/496-pixel-art-icons-for-medievalfantasy-rpg</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>2.<span
            >&nbsp;&nbsp; </span></span><span
            lang=EN>MedicineStorm. Dungeon Crawl 32x32 Tiles Supplemental. License: CC0 -
            Public Domain. Online: <u><span><a
            href="https://opengameart.org/content/dungeon-crawl-32x32-tiles-supplemental"><span
            >https://opengameart.org/content/dungeon-crawl-32x32-tiles-supplemental</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>3.<span
            >&nbsp;&nbsp; </span></span><span
            lang=EN>David E. Gervais. Roguelike Tiles (Large Collection). License: CC-BY
            3.0. Online: <u><span><a
            href="https://opengameart.org/content/roguelike-tiles-large-collection"><span
            >https://opengameart.org/content/roguelike-tiles-large-collection</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>4.<span
            >&nbsp;&nbsp; </span></span><span
            lang=EN>Lanea Zimmerman (Sharm). Tiny 16: Basic. License: CC-BY 3.0. Online: <u><span
            ><a href="https://opengameart.org/content/tiny-16-basic"><span
            >https://opengameart.org/content/tiny-16-basic</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>5.<span
            >&nbsp;&nbsp; </span></span><span
            lang=EN>ArMM1998. Zelda-like Titlesets and Sprites. License: CC0. Online: <u><span
            ><a
            href="https://opengameart.org/content/zelda-like-tilesets-and-sprites"><span
            >https://opengameart.org/content/zelda-like-tilesets-and-sprites</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>6.<span
            >&nbsp;&nbsp; </span></span><span
            lang=EN>Zabin, Hyptosis, Danial Cook. Castle Tiles for RPG's. License: CC-BY
            3.0. Online: <u><span><a
            href="https://opengameart.org/content/castle-tiles-for-rpgs"><span
            >https://opengameart.org/content/castle-tiles-for-rpgs</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpLast style='text-indent:-.25in'><span lang=EN>7.<span
            >&nbsp;&nbsp; </span></span><span
            lang=EN>Zabin, Daneeklu, Jetrel, Hyptosis, Redshrike, Bertram. RPG Tiles:
            Cobble Stone Paths &amp; Town Objects. License: CC-BY-SA 3.0. Online: <u><span
            ><a
            href="https://opengameart.org/content/rpg-tiles-cobble-stone-paths-town-objects"><span
            >https://opengameart.org/content/rpg-tiles-cobble-stone-paths-town-objects</span></a></span></u></span></p>
            </div>

            <h3><a name="_ggf6buxyt3a6"></a><span lang=EN>References</span></h3>

            <div style="padding-left: .5in">
            <p class=MsoListParagraphCxSpFirst style='text-indent:-.25in'><span lang=EN>1.<span
            >&nbsp;&nbsp; </span></span><span
            lang=EN>Hendrickx M., Meijer S., Velden J. V. D., Losup A. (2013) <i>Procedural
            Content Generation for Games: A Survey</i>. ACM Transactions on Multimedia
            Computing, Communications, and Applications. v.9, n.1, p.1-22.' Online Access: <u><span
            ><a href="https://dl.acm.org/citation.cfm?id=2422957"><span
            >https://dl.acm.org/citation.cfm?id=2422957</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>2.<span>&nbsp;&nbsp; </span></span><span
            lang=EN>Horsley L., Perez-Liebana D. (2017) <i>Building an Automatic Sprite
            Generator with Deep Convolutional Generative Adversarial Networks</i>. IEEE
            Conference on Computational Intelligence and Games,' p.134-141. Online Access: <u><span><a
            href="https://ieeexplore.ieee.org/document/8080426/?part=1"><span>
            https://ieeexplore.ieee.org/document/8080426/?part=1</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>3.<span>&nbsp;&nbsp; </span></span><span
            lang=EN>Mirza M., Osindero S. (2014) <i>Conditional Generative Adversarial Nets</i>.
            CoRR. Online Access: <u><span><a
            href="https://arxiv.org/pdf/1411.1784.pdf"><span>https://arxiv.org/pdf/1411.1784.pdf</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>4.<span>&nbsp;&nbsp; </span></span><span
            lang=EN>Radford A., Metz L., Chintala S. (2015) Unsupervised Representation
            Learning with Deep Convolutional Generative Adversarial Networks. CoRR. Online
            Access: <u><span><a
            href="https://arxiv.org/pdf/1511.06434.pdf"><span>https://arxiv.org/pdf/1511.06434.pdf</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'><span lang=EN>5.<span>&nbsp;&nbsp; </span></span><span
            lang=EN>Reed S. E., Zhang Y., Zhang Y., Lee H. (2015) <i>Deep Visual
            Analogy-Making</i>. Advances in Neural Information Processing Systems 28.
            P.1252-1260. Online Access: <u><span><a
            href="http://papers.nips.cc/paper/5845-deep-visual-analogy-making.pdf"><span>
            http://papers.nips.cc/paper/5845-deep-visual-analogy-making.pdf</span></a></span></u></span></p>

            <p class=MsoListParagraphCxSpLast style='text-indent:-.25in'><span lang=EN>6.<span>&nbsp;&nbsp; </span></span><span
            lang=EN>Thanh-Tung H., Tran T., Venkatesh S. (2018) <i>On catastrophic
            forgetting and mode collapse in Generative Adversarial Networks</i>. ICML
            Workshop on Theoretical Foundations and Applications of Deep Generative Models.
            Online Access: <u><span><a
            href="https://arxiv.org/pdf/1807.04015.pdf"><span>https://arxiv.org/pdf/1807.04015.pdf</span></a></span></u></span></p>
            </div>

            </div>

        </div>
    </div>
</section>

        <!-- Contact Section -->
    <section id="contact" class="container content-section text-center contact-section">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Team</h2>
                <p>Our team is composed of students from CS 534 - Computational Photography fall 2018 course at the University of Wisonson Madison. </p>
                <br><br>
                <p>Curt Henrichs (Grad), Sayem Wani (Undergrad), Saheen Feroz (Undergrad)</p>

                <ul class="list-inline banner-social-buttons">
                    
                    <li>
                        <a href="https://github.com/UWMadison-CS534-Term-Project-F2018-CDH" class="btn btn-default btn-lg"><i class="fa fa-github fa-fw"></i> <span class="network-name">github</span></a>
                    </li>
                    
                </ul>
            </div>
        </div>
    </section>

        <!-- Footer -->
    <footer>
        <div class="container text-center">
            <p>Copyright &copy; CNN Sprite Generator - CS 534 Term Project 2018</p>
        </div>
    </footer>

     <!-- jQuery Version 1.11.0 -->
    <script src="/js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="/js/jquery.easing.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="/js/grayscale.js"></script>

    </body>
</html>
